{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df02a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9359ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Projects\\connecteddatahub\\data'\n",
    "grants_path = r'C:\\Projects\\connecteddatahub\\data\\grants\\university_grants.csv'\n",
    "interlocks_path = r'C:\\Projects\\connecteddatahub\\data\\interlocks\\institution_interlocks.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1d5238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AffiliationId', 'carnegie_id', 'PrimarySample', 'FullName', 'SystemId',\n",
      "       'year', 'fullname_norm', 'matched_name', 'num_grants', 'total_funding'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "grants_df = pd.read_csv(grants_path)\n",
    "print(grants_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22459898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'AffiliationId', 'Institution', 'TotalInterlocks', 'BoardSize'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "interlocks_df = pd.read_csv(interlocks_path)\n",
    "interlocks_df = interlocks_df[interlocks_df['Year'].astype(int) >= 2010]\n",
    "print(interlocks_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d0ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'AffiliationId', 'Institution', 'TotalInterlocks', 'BoardSize',\n",
      "       'num_grants', 'total_funding'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "grants_df[\"year\"] = grants_df[\"year\"].astype(str)\n",
    "interlocks_df[\"Year\"] = interlocks_df[\"Year\"].astype(str)\n",
    "\n",
    "# Merge grant data onto interlocks_df\n",
    "merged_df = interlocks_df.merge(\n",
    "    grants_df[[\"AffiliationId\", \"year\", \"num_grants\", \"total_funding\"]],\n",
    "    how=\"left\",\n",
    "    left_on=[\"AffiliationId\", \"Year\"],\n",
    "    right_on=[\"AffiliationId\", \"year\"]\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"year\"])\n",
    "\n",
    "merged_df[\"num_grants\"] = merged_df[\"num_grants\"].fillna(0).astype(int)\n",
    "merged_df[\"total_funding\"] = merged_df[\"total_funding\"].fillna(0)\n",
    "\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0e4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'AffiliationId_1', 'Institution_1', 'AffiliationId_2',\n",
      "       'Institution_2', 'InterlockCount', 'BoardSize_1', 'BoardSize_2',\n",
      "       'WeightNorm'],\n",
      "      dtype='object')\n",
      "   Year  AffiliationId_1                           Institution_1  \\\n",
      "0  1999             1003              Tennessee Board Of Regents   \n",
      "1  1999             1003              Tennessee Board Of Regents   \n",
      "2  1999           392282  State University Of New York At Albany   \n",
      "3  1999          1174212       University Of Southern California   \n",
      "4  1999          1174212       University Of Southern California   \n",
      "5  1999          1629065             Central Michigan University   \n",
      "6  1999          5388228                 University Of Rochester   \n",
      "7  1999          5388228                 University Of Rochester   \n",
      "8  1999          5388228                 University Of Rochester   \n",
      "9  1999          5388228                 University Of Rochester   \n",
      "\n",
      "   AffiliationId_2                   Institution_2  InterlockCount  \\\n",
      "0        200719446           Vanderbilt University               1   \n",
      "1       2799495847  University Of Tennessee System              15   \n",
      "2         71965598              Adelphi University               1   \n",
      "3         74973139      Carnegie Mellon University               1   \n",
      "4         86501945          University Of Delaware               1   \n",
      "5         87208437             University Of Tulsa               1   \n",
      "6         40347166           University Of Chicago               1   \n",
      "7         66108857            Gallaudet University               1   \n",
      "8         87208437             University Of Tulsa               1   \n",
      "9        145311948        Johns Hopkins University               1   \n",
      "\n",
      "   BoardSize_1  BoardSize_2  WeightNorm  \n",
      "0           14           33    0.021277  \n",
      "1           14           15    0.517241  \n",
      "2           10           16    0.038462  \n",
      "3           34           60    0.010638  \n",
      "4           34           28    0.016129  \n",
      "5            7           30    0.027027  \n",
      "6           51           42    0.010753  \n",
      "7           51           17    0.014706  \n",
      "8           51           30    0.012346  \n",
      "9           51           53    0.009615  \n"
     ]
    }
   ],
   "source": [
    "interlock_edges_path = r'C:\\Projects\\connecteddatahub\\data\\interlocks\\yearly_interlocks.csv'\n",
    "yearly_edges_df = pd.read_csv(interlock_edges_path)\n",
    "print(yearly_edges_df.columns)\n",
    "print(yearly_edges_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba8553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52670e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "# import statsmodels.api as sm\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# # =======================================\n",
    "# # 0. Compute yearly network centrality metrics\n",
    "# # =======================================\n",
    "# centrality_rows = []\n",
    "\n",
    "# for year, sub_edges in yearly_edges_df.groupby(\"Year\"):\n",
    "#     G = nx.Graph()\n",
    "#     for _, row in sub_edges.iterrows():\n",
    "#         G.add_edge(row[\"AffiliationId_1\"], row[\"AffiliationId_2\"], weight=row[\"WeightNorm\"])\n",
    "\n",
    "#     # Compute metrics\n",
    "#     degree = nx.degree_centrality(G)\n",
    "#     weighted_degree = dict(G.degree(weight=\"WeightNorm\"))\n",
    "#     betw = nx.betweenness_centrality(G, weight=\"WeightNorm\", normalized=True)\n",
    "#     clust = nx.clustering(G, weight=\"WeightNorm\")\n",
    "\n",
    "#     for node in G.nodes():\n",
    "#         centrality_rows.append({\n",
    "#             \"Year\": str(year),\n",
    "#             \"AffiliationId\": node,\n",
    "#             \"degree_centrality\": degree.get(node, 0),\n",
    "#             \"weighted_degree\": weighted_degree.get(node, 0),\n",
    "#             \"betweenness_centrality\": betw.get(node, 0),\n",
    "#             \"clustering_coeff\": clust.get(node, 0),\n",
    "#         })\n",
    "\n",
    "# centrality_df = pd.DataFrame(centrality_rows)\n",
    "# print(f\"Centrality metrics computed for {len(centrality_df)} institution-year pairs\")\n",
    "\n",
    "# # Merge metrics into main merged_df\n",
    "# df = merged_df.merge(centrality_df, on=[\"Year\", \"AffiliationId\"], how=\"left\")\n",
    "\n",
    "# # =======================================\n",
    "# # 1. Data preparation\n",
    "# # =======================================\n",
    "# for c in [\n",
    "#     \"TotalInterlocks\", \"BoardSize\", \"num_grants\", \"total_funding\",\n",
    "#     \"degree_centrality\", \"weighted_degree\", \"betweenness_centrality\", \"clustering_coeff\"\n",
    "# ]:\n",
    "#     df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# df[\"log_funding\"] = np.log1p(df[\"total_funding\"])\n",
    "\n",
    "# # =======================================\n",
    "# # 2. Basic correlations\n",
    "# # =======================================\n",
    "# print(\"Spearman correlations:\")\n",
    "# corr = df[[\n",
    "#     \"TotalInterlocks\", \"BoardSize\", \"degree_centrality\", \"weighted_degree\",\n",
    "#     \"betweenness_centrality\", \"clustering_coeff\", \"num_grants\", \"log_funding\"\n",
    "# ]].corr(method=\"spearman\")\n",
    "# print(corr, \"\\n\")\n",
    "\n",
    "# sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "# plt.title(\"Spearman correlation matrix (with network metrics)\")\n",
    "# plt.show()\n",
    "\n",
    "# # =======================================\n",
    "# # 3. Simple OLS regression (pooled)\n",
    "# # =======================================\n",
    "# ols_formula = (\n",
    "#     \"log_funding ~ TotalInterlocks + BoardSize + \"\n",
    "#     \"degree_centrality + weighted_degree + \"\n",
    "#     \"betweenness_centrality + clustering_coeff\"\n",
    "# )\n",
    "# ols_model = smf.ols(ols_formula, data=df).fit(cov_type=\"HC3\")\n",
    "# print(ols_model.summary())\n",
    "\n",
    "# # =======================================\n",
    "# # 4. Quantile regression (localized effects)\n",
    "# # =======================================\n",
    "# quant_formula = (\n",
    "#     \"log_funding ~ TotalInterlocks + BoardSize + \"\n",
    "#     \"degree_centrality + weighted_degree + \"\n",
    "#     \"betweenness_centrality + clustering_coeff\"\n",
    "# )\n",
    "# quant_mod = smf.quantreg(quant_formula, df)\n",
    "\n",
    "# for q in [0.25, 0.5, 0.75, 0.9]:\n",
    "#     res = quant_mod.fit(q=q)\n",
    "#     print(f\"\\nQuantile {q} results:\")\n",
    "#     print(res.summary())\n",
    "\n",
    "# # =======================================\n",
    "# # 5. Visualization: local associations\n",
    "# # =======================================\n",
    "# sns.lmplot(\n",
    "#     x=\"weighted_degree\", y=\"log_funding\",\n",
    "#     data=df, lowess=True, scatter_kws={\"alpha\":0.3}, line_kws={\"color\":\"red\"}\n",
    "# )\n",
    "# plt.title(\"Local relationship: Weighted Degree vs log(Funding)\")\n",
    "# plt.show()\n",
    "\n",
    "# sns.lmplot(\n",
    "#     x=\"betweenness_centrality\", y=\"log_funding\",\n",
    "#     data=df, lowess=True, scatter_kws={\"alpha\":0.3}, line_kws={\"color\":\"blue\"}\n",
    "# )\n",
    "# plt.title(\"Local relationship: Betweenness Centrality vs log(Funding)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24dd811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from networkx.algorithms import community\n",
    "# from esda.moran import Moran\n",
    "# from libpysal.weights import WSP, full2W\n",
    "\n",
    "# print(\"=== Local Structure Tests ===\\n\")\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for yr, sub_edges in yearly_edges_df.groupby(\"Year\"):\n",
    "#     # Build weighted graph for this year\n",
    "#     G = nx.Graph()\n",
    "#     for _, r in sub_edges.iterrows():\n",
    "#         G.add_edge(r[\"AffiliationId_1\"], r[\"AffiliationId_2\"], weight=r[\"WeightNorm\"])\n",
    "\n",
    "#     # Skip empty years\n",
    "#     if G.number_of_nodes() < 3:\n",
    "#         continue\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 1. Community detection\n",
    "#     # ---------------------------\n",
    "#     comms = community.greedy_modularity_communities(G, weight=\"WeightNorm\")\n",
    "#     node_to_comm = {n: i for i, c in enumerate(comms) for n in c}\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 2. Neighbor funding stats\n",
    "#     # ---------------------------\n",
    "#     fund_map = merged_df[merged_df[\"Year\"] == str(yr)].set_index(\"AffiliationId\")[\"total_funding\"].to_dict()\n",
    "#     neighbor_stats = []\n",
    "#     for n in G.nodes():\n",
    "#         neighbor_funds = [fund_map.get(v, 0) for v in G.neighbors(n)]\n",
    "#         mean_neigh_fund = np.mean(neighbor_funds) if neighbor_funds else 0\n",
    "#         neighbor_stats.append((n, mean_neigh_fund))\n",
    "\n",
    "#     neigh_df = pd.DataFrame(neighbor_stats, columns=[\"AffiliationId\", \"mean_neighbor_funding\"])\n",
    "#     neigh_df[\"Year\"] = str(yr)\n",
    "#     neigh_df[\"community_id\"] = neigh_df[\"AffiliationId\"].map(node_to_comm)\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 3. Moran’s I test\n",
    "#     # ---------------------------\n",
    "#     nodes = list(G.nodes())\n",
    "#     node_idx = {n: i for i, n in enumerate(nodes)}\n",
    "#     W_mat = np.zeros((len(nodes), len(nodes)))\n",
    "#     for u, v, d in G.edges(data=True):\n",
    "#         i, j = node_idx[u], node_idx[v]\n",
    "#         W_mat[i, j] = W_mat[j, i] = d.get(\"weight\", 1.0)\n",
    "\n",
    "#     # Convert adjacency to PySAL weight object\n",
    "#     try:\n",
    "#         W_obj = full2W(W_mat)\n",
    "#     except Exception:\n",
    "#         # fallback to sparse representation\n",
    "#         W_obj = WSP(W_mat)\n",
    "\n",
    "#     funding_vec = np.array([fund_map.get(n, 0) for n in nodes])\n",
    "\n",
    "#     moran = Moran(funding_vec, W_obj)\n",
    "#     results.append({\n",
    "#         \"Year\": yr,\n",
    "#         \"NumNodes\": G.number_of_nodes(),\n",
    "#         \"NumEdges\": G.number_of_edges(),\n",
    "#         \"Communities\": len(comms),\n",
    "#         \"Moran_I\": moran.I,\n",
    "#         \"Moran_p\": moran.p_sim\n",
    "#     })\n",
    "\n",
    "#     print(f\"Year {yr}:\")\n",
    "#     print(f\"  Nodes={G.number_of_nodes()}  Edges={G.number_of_edges()}  Communities={len(comms)}\")\n",
    "#     print(f\"  Moran’s I = {moran.I:.3f}  (p = {moran.p_sim:.4f})\")\n",
    "#     print(\"  Example communities:\", [len(c) for c in comms[:5]], \"\\n\")\n",
    "\n",
    "# results_df = pd.DataFrame(results)\n",
    "# print(\"\\n=== Summary of Moran’s I by year ===\")\n",
    "# print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
