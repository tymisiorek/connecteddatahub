{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754a318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3ea328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Name', 'Position', 'Institution', 'SubInstitution',\n",
      "       'Education', 'FixedPosition', 'Seniority', 'Designation',\n",
      "       'AffiliationId', 'carnegie_id', 'SystemId', 'RawName', 'RawInstitution',\n",
      "       'RawPosition', 'FirstName', 'MiddleName', 'LastName', 'NickName',\n",
      "       'SuffixName', 'PrefixName', 'MiddleInitials', 'RawName4Match',\n",
      "       'LeaderId'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "leadership_path = r'C:\\Projects\\connecteddatahub\\data\\cleaned_dataframes\\master_leadershipdata.csv'\n",
    "data_path = r'C:\\Projects\\connecteddatahub\\data'\n",
    "leadership_df = pd.read_csv(leadership_path)\n",
    "print(leadership_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8707a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54153, 24)\n"
     ]
    }
   ],
   "source": [
    "board_df =  leadership_df[leadership_df['FixedPosition'] == 'Board Member']\n",
    "print(board_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a210ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_interlock_graphs(board_df):\n",
    "#     \"\"\"Create node and edge DataFrames for yearly interlocks.\"\"\"\n",
    "#     interlocks = []\n",
    "\n",
    "#     # person-level pairing\n",
    "#     for (year, first, last), person_data in board_df.groupby([\"Year\", \"FirstName\", \"LastName\"]):\n",
    "#         affiliations = person_data[[\"AffiliationId\", \"Institution\", \"SystemId\"]].drop_duplicates()\n",
    "#         if len(affiliations) <= 1:\n",
    "#             continue\n",
    "\n",
    "#         for (id1, inst1, sys1), (id2, inst2, sys2) in combinations(affiliations.values, 2):\n",
    "#             if pd.notna(sys1) and pd.notna(sys2) and sys1 == sys2:\n",
    "#                 continue\n",
    "#             u, v = sorted((id1, id2))\n",
    "#             interlocks.append({\n",
    "#                 \"Year\": year,\n",
    "#                 \"AffiliationId_1\": u,\n",
    "#                 \"AffiliationId_2\": v,\n",
    "#                 \"Institution_1\": inst1 if u == id1 else inst2,\n",
    "#                 \"Institution_2\": inst2 if u == id1 else inst1,\n",
    "#                 \"SystemId_1\": sys1 if u == id1 else sys2,\n",
    "#                 \"SystemId_2\": sys2 if u == id1 else sys1,\n",
    "#             })\n",
    "\n",
    "#     edges = pd.DataFrame(interlocks)\n",
    "\n",
    "#     # edge aggregation\n",
    "#     edges = (\n",
    "#         edges.groupby([\"Year\", \"AffiliationId_1\", \"Institution_1\",\n",
    "#                        \"AffiliationId_2\", \"Institution_2\"], as_index=False)\n",
    "#         .size()\n",
    "#         .rename(columns={\"size\": \"InterlockCount\"})\n",
    "#     )\n",
    "\n",
    "#     # board sizes and normalized weight\n",
    "#     board_sizes = (\n",
    "#         board_df.groupby([\"Year\", \"AffiliationId\"])[\"Name\"]\n",
    "#         .nunique()\n",
    "#         .reset_index(name=\"BoardSize\")\n",
    "#     )\n",
    "\n",
    "#     edges = edges.merge(\n",
    "#         board_sizes.rename(columns={\"AffiliationId\": \"AffiliationId_1\", \"BoardSize\": \"BoardSize_1\"}),\n",
    "#         on=[\"Year\", \"AffiliationId_1\"], how=\"left\"\n",
    "#     ).merge(\n",
    "#         board_sizes.rename(columns={\"AffiliationId\": \"AffiliationId_2\", \"BoardSize\": \"BoardSize_2\"}),\n",
    "#         on=[\"Year\", \"AffiliationId_2\"], how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     edges[\"WeightNorm\"] = edges[\"InterlockCount\"] / (\n",
    "#         edges[\"BoardSize_1\"] + edges[\"BoardSize_2\"]\n",
    "#     )\n",
    "\n",
    "#     # node list\n",
    "#     nodes = (\n",
    "#         board_df[[\"AffiliationId\", \"Institution\", \"SystemId\"]]\n",
    "#         .drop_duplicates()\n",
    "#         .rename(columns={\"AffiliationId\": \"Id\", \"Institution\": \"Label\"})\n",
    "#     )\n",
    "\n",
    "#     return edges, nodes\n",
    "\n",
    "\n",
    "# def compute_edge_weakness(G, alpha=0.7, beta=0.3):\n",
    "#     \"\"\"Assign 'Weakness' to each edge as 0.7*(1 - overlap) + 0.3*betweenness.\"\"\"\n",
    "#     # neighborhood overlap\n",
    "#     overlap = {}\n",
    "#     for u, v in G.edges():\n",
    "#         Nu, Nv = set(G.neighbors(u)) - {v}, set(G.neighbors(v)) - {u}\n",
    "#         inter = len(Nu & Nv)\n",
    "#         denom = len(Nu | Nv)\n",
    "#         overlap[(u, v)] = inter / (denom - 2) if denom > 2 else 0.0\n",
    "#     nx.set_edge_attributes(G, overlap, \"overlap\")\n",
    "\n",
    "#     # edge betweenness\n",
    "#     betweenness = nx.edge_betweenness_centrality(G, normalized=True)\n",
    "#     nx.set_edge_attributes(G, betweenness, \"betweenness\")\n",
    "\n",
    "#     # combined weakness\n",
    "#     weakness = {\n",
    "#         (u, v): alpha * (1 - d.get(\"overlap\", 0.0)) + beta * d.get(\"betweenness\", 0.0)\n",
    "#         for u, v, d in G.edges(data=True)\n",
    "#     }\n",
    "#     nx.set_edge_attributes(G, weakness, \"weakness\")\n",
    "#     return weakness\n",
    "\n",
    "\n",
    "# def attach_weakness(edges, nodes, alpha=0.7, beta=0.3):\n",
    "#     \"\"\"\n",
    "#     Compute and attach edge weaknesses for each year.\n",
    "#     Ensures canonical ordering (min, max) for merge alignment.\n",
    "#     \"\"\"\n",
    "#     # enforce consistent dtypes (prevents merge misses due to int vs str)\n",
    "#     edges = edges.copy()\n",
    "#     edges[\"AffiliationId_1\"] = edges[\"AffiliationId_1\"].astype(object)\n",
    "#     edges[\"AffiliationId_2\"] = edges[\"AffiliationId_2\"].astype(object)\n",
    "\n",
    "#     all_weakness = []\n",
    "\n",
    "#     for year, group in edges.groupby(\"Year\", sort=False):\n",
    "#         if group.empty:\n",
    "#             continue\n",
    "\n",
    "#         # Build undirected graph from this year's edges\n",
    "#         G = nx.from_pandas_edgelist(\n",
    "#             group,\n",
    "#             source=\"AffiliationId_1\",\n",
    "#             target=\"AffiliationId_2\",\n",
    "#             edge_attr=\"WeightNorm\",\n",
    "#             create_using=nx.Graph(),\n",
    "#         )\n",
    "\n",
    "#         if G.number_of_edges() == 0:\n",
    "#             continue\n",
    "\n",
    "#         compute_edge_weakness(G, alpha, beta)\n",
    "\n",
    "#         # IMPORTANT: canonicalize (min, max) before creating the weakness rows\n",
    "#         rows = []\n",
    "#         for u, v, d in G.edges(data=True):\n",
    "#             a, b = (u, v) if u <= v else (v, u)\n",
    "#             rows.append({\n",
    "#                 \"Year\": year,\n",
    "#                 \"AffiliationId_1\": a,\n",
    "#                 \"AffiliationId_2\": b,\n",
    "#                 \"Weakness\": d.get(\"weakness\", np.nan)\n",
    "#             })\n",
    "\n",
    "#         all_weakness.append(pd.DataFrame(rows))\n",
    "\n",
    "#     if not all_weakness:\n",
    "#         # nothing computed; return edges with a Weakness column of NaN\n",
    "#         edges[\"Weakness\"] = np.nan\n",
    "#         return edges\n",
    "\n",
    "#     weakness_df = pd.concat(all_weakness, ignore_index=True)\n",
    "\n",
    "#     # Merge by canonical IDs + Year\n",
    "#     out = edges.merge(\n",
    "#         weakness_df,\n",
    "#         on=[\"Year\", \"AffiliationId_1\", \"AffiliationId_2\"],\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329af549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:338: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:338: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\tykun\\AppData\\Local\\Temp\\ipykernel_30244\\3014831229.py:338: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def _eigvec_centrality_componentwise(G, weight=\"WeightNorm\", max_iter=2000, tol=1e-07):\n",
    "    \"\"\"\n",
    "    Robust eigenvector centrality for disconnected graphs:\n",
    "    - Run per connected component (undirected), weighted if available.\n",
    "    - Isolated nodes get 0.0.\n",
    "    \"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return {}\n",
    "\n",
    "    scores = {n: 0.0 for n in G.nodes()}  # default for isolates / tiny comps\n",
    "\n",
    "    for comp_nodes in nx.connected_components(G):\n",
    "        H = G.subgraph(comp_nodes).copy()\n",
    "        # If the component has no edges (single node), leave score=0.0\n",
    "        if H.number_of_edges() == 0:\n",
    "            continue\n",
    "        # Try numpy (fast, stable on connected graphs)\n",
    "        try:\n",
    "            part = nx.eigenvector_centrality_numpy(H, weight=weight)\n",
    "        except nx.AmbiguousSolution:\n",
    "            # Shouldn't happen per-component, but keep belt-and-suspenders\n",
    "            part = nx.eigenvector_centrality(H, weight=weight, max_iter=max_iter, tol=tol)\n",
    "        except Exception:\n",
    "            # Fallback to power iteration with relaxed params\n",
    "            part = nx.eigenvector_centrality(H, weight=weight, max_iter=max_iter, tol=tol)\n",
    "        # Write back\n",
    "        for n, v in part.items():\n",
    "            scores[n] = float(v)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def _safe_betweenness(G, use_weighted_betweenness=False, weight=\"WeightNorm\"):\n",
    "    \"\"\"\n",
    "    Betweenness that optionally uses distance=1/weight. Works for disconnected graphs.\n",
    "    \"\"\"\n",
    "    if not use_weighted_betweenness:\n",
    "        return nx.betweenness_centrality(G, normalized=True)\n",
    "    eps = 1e-12\n",
    "    inv_dist = {(u, v): 1.0 / max(float(d.get(weight, 0.0)), eps) for u, v, d in G.edges(data=True)}\n",
    "    nx.set_edge_attributes(G, inv_dist, name=\"_dist\")\n",
    "    bt = nx.betweenness_centrality(G, normalized=True, weight=\"_dist\")\n",
    "    # cleanup\n",
    "    for u, v in G.edges():\n",
    "        if \"_dist\" in G[u][v]:\n",
    "            del G[u][v][\"_dist\"]\n",
    "    return bt\n",
    "\n",
    "\n",
    "\n",
    "def build_interlock_graphs(board_df):\n",
    "    \"\"\"Create node and edge DataFrames for yearly interlocks.\"\"\"\n",
    "    interlocks = []\n",
    "\n",
    "    # person-level pairing\n",
    "    for (year, first, last), person_data in board_df.groupby([\"Year\", \"FirstName\", \"LastName\"]):\n",
    "        affiliations = person_data[[\"AffiliationId\", \"Institution\", \"SystemId\"]].drop_duplicates()\n",
    "        if len(affiliations) <= 1:\n",
    "            continue\n",
    "\n",
    "        for (id1, inst1, sys1), (id2, inst2, sys2) in combinations(affiliations.values, 2):\n",
    "            # Skip if both belong to the same system\n",
    "            if pd.notna(sys1) and pd.notna(sys2) and sys1 == sys2:\n",
    "                continue\n",
    "\n",
    "            # Canonicalize undirected edge\n",
    "            u, v = sorted((id1, id2))\n",
    "            interlocks.append({\n",
    "                \"Year\": year,\n",
    "                \"AffiliationId_1\": u,\n",
    "                \"AffiliationId_2\": v,\n",
    "                \"Institution_1\": inst1 if u == id1 else inst2,\n",
    "                \"Institution_2\": inst2 if u == id1 else inst1,\n",
    "                \"SystemId_1\": sys1 if u == id1 else sys2,\n",
    "                \"SystemId_2\": sys2 if u == id1 else sys1,\n",
    "            })\n",
    "\n",
    "    edges = pd.DataFrame(interlocks)\n",
    "\n",
    "    # edge aggregation\n",
    "    edges = (\n",
    "        edges.groupby([\"Year\", \"AffiliationId_1\", \"Institution_1\",\n",
    "                       \"AffiliationId_2\", \"Institution_2\"], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"InterlockCount\"})\n",
    "    )\n",
    "\n",
    "    # board sizes and normalized weight\n",
    "    board_sizes = (\n",
    "        board_df.groupby([\"Year\", \"AffiliationId\"])[\"Name\"]\n",
    "        .nunique()\n",
    "        .reset_index(name=\"BoardSize\")\n",
    "    )\n",
    "\n",
    "    edges = edges.merge(\n",
    "        board_sizes.rename(columns={\"AffiliationId\": \"AffiliationId_1\", \"BoardSize\": \"BoardSize_1\"}),\n",
    "        on=[\"Year\", \"AffiliationId_1\"], how=\"left\"\n",
    "    ).merge(\n",
    "        board_sizes.rename(columns={\"AffiliationId\": \"AffiliationId_2\", \"BoardSize\": \"BoardSize_2\"}),\n",
    "        on=[\"Year\", \"AffiliationId_2\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    edges[\"WeightNorm\"] = edges[\"InterlockCount\"] / (\n",
    "        edges[\"BoardSize_1\"] + edges[\"BoardSize_2\"]\n",
    "    )\n",
    "\n",
    "    # node list (year-invariant; we'll add per-year metrics later)\n",
    "    nodes = (\n",
    "        board_df[[\"AffiliationId\", \"Institution\", \"SystemId\"]]\n",
    "        .drop_duplicates()\n",
    "        .rename(columns={\"AffiliationId\": \"Id\", \"Institution\": \"Label\"})\n",
    "    )\n",
    "\n",
    "    return edges, nodes\n",
    "\n",
    "\n",
    "def compute_edge_weakness(G, alpha=0.7, beta=0.3):\n",
    "    \"\"\"Assign 'Weakness' to each edge as 0.7*(1 - overlap) + 0.3*betweenness.\"\"\"\n",
    "    # neighborhood overlap\n",
    "    overlap = {}\n",
    "    for u, v in G.edges():\n",
    "        Nu, Nv = set(G.neighbors(u)) - {v}, set(G.neighbors(v)) - {u}\n",
    "        inter = len(Nu & Nv)\n",
    "        denom = len(Nu | Nv)\n",
    "        overlap[(u, v)] = inter / (denom - 2) if denom > 2 else 0.0\n",
    "    nx.set_edge_attributes(G, overlap, \"overlap\")\n",
    "\n",
    "    # edge betweenness\n",
    "    betweenness = nx.edge_betweenness_centrality(G, normalized=True)\n",
    "    nx.set_edge_attributes(G, betweenness, \"betweenness\")\n",
    "\n",
    "    # combined weakness\n",
    "    weakness = {\n",
    "        (u, v): alpha * (1 - d.get(\"overlap\", 0.0)) + beta * d.get(\"betweenness\", 0.0)\n",
    "        for u, v, d in G.edges(data=True)\n",
    "    }\n",
    "    nx.set_edge_attributes(G, weakness, \"weakness\")\n",
    "    return weakness\n",
    "\n",
    "\n",
    "def attach_weakness(edges, nodes, alpha=0.7, beta=0.3):\n",
    "    \"\"\"\n",
    "    Compute and attach edge weaknesses for each year.\n",
    "    Ensures canonical ordering (min, max) for merge alignment.\n",
    "    \"\"\"\n",
    "    edges = edges.copy()\n",
    "    edges[\"AffiliationId_1\"] = edges[\"AffiliationId_1\"].astype(object)\n",
    "    edges[\"AffiliationId_2\"] = edges[\"AffiliationId_2\"].astype(object)\n",
    "\n",
    "    all_weakness = []\n",
    "\n",
    "    for year, group in edges.groupby(\"Year\", sort=False):\n",
    "        if group.empty:\n",
    "            continue\n",
    "\n",
    "        # Build undirected graph from this year's edges\n",
    "        G = nx.from_pandas_edgelist(\n",
    "            group,\n",
    "            source=\"AffiliationId_1\",\n",
    "            target=\"AffiliationId_2\",\n",
    "            edge_attr=\"WeightNorm\",\n",
    "            create_using=nx.Graph(),\n",
    "        )\n",
    "\n",
    "        if G.number_of_edges() == 0:\n",
    "            continue\n",
    "\n",
    "        compute_edge_weakness(G, alpha, beta)\n",
    "\n",
    "        rows = []\n",
    "        for u, v, d in G.edges(data=True):\n",
    "            a, b = (u, v) if u <= v else (v, u)\n",
    "            rows.append({\n",
    "                \"Year\": year,\n",
    "                \"AffiliationId_1\": a,\n",
    "                \"AffiliationId_2\": b,\n",
    "                \"Weakness\": d.get(\"weakness\", np.nan)\n",
    "            })\n",
    "        all_weakness.append(pd.DataFrame(rows))\n",
    "\n",
    "    if not all_weakness:\n",
    "        edges[\"Weakness\"] = np.nan\n",
    "        return edges\n",
    "\n",
    "    weakness_df = pd.concat(all_weakness, ignore_index=True)\n",
    "\n",
    "    out = edges.merge(\n",
    "        weakness_df,\n",
    "        on=[\"Year\", \"AffiliationId_1\", \"AffiliationId_2\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) NEW: attach_node_metrics -> returns NODES DF (by Year)\n",
    "# =========================================================\n",
    "\n",
    "def _shannon_entropy_bits(values):\n",
    "    \"\"\"Base-2 Shannon entropy over nonnegative weights.\"\"\"\n",
    "    vals = np.asarray(list(values), dtype=float)\n",
    "    s = vals.sum()\n",
    "    if s <= 0 or vals.size == 0:\n",
    "        return 0.0\n",
    "    p = vals / s\n",
    "    p = p[p > 0]\n",
    "    return float(-np.sum(p * np.log2(p)))\n",
    "\n",
    "\n",
    "def attach_node_metrics(edges, nodes, weight_col=\"WeightNorm\", use_weighted_betweenness=False):\n",
    "    \"\"\"\n",
    "    Compute node metrics per Year and return an UPDATED nodes DataFrame.\n",
    "    Output is one row per (Year, Id) with node attributes + metrics.\n",
    "\n",
    "    Metrics added:\n",
    "      - degree\n",
    "      - degree_centrality\n",
    "      - eigenvector_centrality (weighted by `weight_col`)\n",
    "      - betweenness_centrality (optionally weighted via distance = 1/weight)\n",
    "      - clustering (local, unweighted)\n",
    "      - modularity_class (greedy modularity)\n",
    "      - interlock_entropy_bits (entropy over incident edge weights)\n",
    "      - strength (sum of incident edge weights)\n",
    "    \"\"\"\n",
    "    if edges.empty:\n",
    "        base_cols = [\"Year\",\"Id\",\"Label\",\"SystemId\",\"degree\",\"degree_centrality\",\n",
    "                     \"eigenvector_centrality\",\"betweenness_centrality\",\n",
    "                     \"clustering\",\"modularity_class\",\"interlock_entropy_bits\",\"strength\"]\n",
    "        return pd.DataFrame(columns=base_cols)\n",
    "\n",
    "    edges = edges.copy()\n",
    "    edges[\"AffiliationId_1\"] = edges[\"AffiliationId_1\"].astype(object)\n",
    "    edges[\"AffiliationId_2\"] = edges[\"AffiliationId_2\"].astype(object)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for year, gdf in edges.groupby(\"Year\", sort=False):\n",
    "        if gdf.empty:\n",
    "            continue\n",
    "\n",
    "        edge_attr = [weight_col] if (weight_col in gdf.columns) else None\n",
    "        G = nx.from_pandas_edgelist(\n",
    "            gdf,\n",
    "            source=\"AffiliationId_1\",\n",
    "            target=\"AffiliationId_2\",\n",
    "            edge_attr=edge_attr,\n",
    "            create_using=nx.Graph()\n",
    "        )\n",
    "        if G.number_of_nodes() == 0:\n",
    "            continue\n",
    "\n",
    "        # Degree & degree centrality\n",
    "        degree = dict(G.degree())\n",
    "        degree_c = nx.degree_centrality(G)\n",
    "\n",
    "        eig_c = _eigvec_centrality_componentwise(G, weight=weight_col, max_iter=2000, tol=1e-7)\n",
    "        betw = _safe_betweenness(G, use_weighted_betweenness=use_weighted_betweenness, weight=weight_col)\n",
    "\n",
    "        # Betweenness (optionally weighted via distance = 1/weight)\n",
    "        if use_weighted_betweenness and G.number_of_edges() > 0:\n",
    "            eps = 1e-12\n",
    "            inv_dist = {(u, v): 1.0 / max(float(d.get(weight_col, 0.0)), eps) for u, v, d in G.edges(data=True)}\n",
    "            nx.set_edge_attributes(G, inv_dist, name=\"_dist\")\n",
    "            betw = nx.betweenness_centrality(G, normalized=True, weight=\"_dist\")\n",
    "            for u, v in G.edges():\n",
    "                if \"_dist\" in G[u][v]:\n",
    "                    del G[u][v][\"_dist\"]\n",
    "        else:\n",
    "            betw = nx.betweenness_centrality(G, normalized=True)\n",
    "\n",
    "        # Local clustering (unweighted)\n",
    "        clust = nx.clustering(G)\n",
    "\n",
    "        # Greedy modularity -> integer community labels\n",
    "        communities = list(nx.algorithms.community.greedy_modularity_communities(G, weight=weight_col))\n",
    "        node2comm = {}\n",
    "        for cid, comm in enumerate(communities):\n",
    "            for n in comm:\n",
    "                node2comm[n] = cid\n",
    "\n",
    "        # Strength + entropy over incident weights\n",
    "        strength = {}\n",
    "        entropy_bits = {}\n",
    "        for n in G.nodes():\n",
    "            nbr_w = [float(G[n][nbr].get(weight_col, 1.0)) for nbr in G.neighbors(n)]\n",
    "            strength[n] = float(np.sum(nbr_w)) if nbr_w else 0.0\n",
    "            entropy_bits[n] = _shannon_entropy_bits(nbr_w)\n",
    "\n",
    "        # Assemble rows (Year, Id)\n",
    "        rows = []\n",
    "        for n in G.nodes():\n",
    "            rows.append({\n",
    "                \"Year\": year,\n",
    "                \"Id\": n,\n",
    "                \"degree\": degree.get(n, 0),\n",
    "                \"degree_centrality\": degree_c.get(n, 0.0),\n",
    "                \"eigenvector_centrality\": eig_c.get(n, 0.0),\n",
    "                \"betweenness_centrality\": betw.get(n, 0.0),\n",
    "                \"clustering\": clust.get(n, 0.0),\n",
    "                \"modularity_class\": node2comm.get(n, -1),\n",
    "                \"interlock_entropy_bits\": entropy_bits.get(n, 0.0),\n",
    "                \"strength\": strength.get(n, 0.0),\n",
    "            })\n",
    "        out.append(pd.DataFrame(rows))\n",
    "\n",
    "    if not out:\n",
    "        base_cols = [\"Year\",\"Id\",\"Label\",\"SystemId\",\"degree\",\"degree_centrality\",\n",
    "                     \"eigenvector_centrality\",\"betweenness_centrality\",\n",
    "                     \"clustering\",\"modularity_class\",\"interlock_entropy_bits\",\"strength\"]\n",
    "        return pd.DataFrame(columns=base_cols)\n",
    "\n",
    "    metrics = pd.concat(out, ignore_index=True)\n",
    "\n",
    "    # Join node attributes (Label, SystemId) -> single nodes_df with Year granularity\n",
    "    node_attrs = nodes[[\"Id\", \"Label\", \"SystemId\"]].drop_duplicates()\n",
    "    nodes_updated = (\n",
    "        metrics.merge(node_attrs, on=\"Id\", how=\"left\")\n",
    "        .reindex(columns=[\"Year\",\"Id\",\"Label\",\"SystemId\",\n",
    "                          \"degree\",\"degree_centrality\",\"eigenvector_centrality\",\n",
    "                          \"betweenness_centrality\",\"clustering\",\"modularity_class\",\n",
    "                          \"interlock_entropy_bits\",\"strength\"])\n",
    "        .sort_values([\"Year\",\"Id\"], kind=\"mergesort\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return nodes_updated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def attach_edge_entropy(edges, weight_col=\"WeightNorm\", out_col=\"EdgeEntropyBits\"):\n",
    "    \"\"\"\n",
    "    Compute edge-level local entropy around endpoints and return UPDATED edges DF.\n",
    "    For each (Year, u, v): H( concat( weights(u,·\\{v}), weights(v,·\\{u}) ) ).\n",
    "    If no neighbors besides each other, entropy=0.\n",
    "    \"\"\"\n",
    "    if edges.empty:\n",
    "        edges[out_col] = np.nan\n",
    "        return edges\n",
    "\n",
    "    edges = edges.copy()\n",
    "    edges[\"AffiliationId_1\"] = edges[\"AffiliationId_1\"].astype(object)\n",
    "    edges[\"AffiliationId_2\"] = edges[\"AffiliationId_2\"].astype(object)\n",
    "\n",
    "    out_chunks = []\n",
    "    for year, gdf in edges.groupby(\"Year\", sort=False):\n",
    "        edge_attr = [weight_col] if (weight_col in gdf.columns) else None\n",
    "        G = nx.from_pandas_edgelist(\n",
    "            gdf,\n",
    "            source=\"AffiliationId_1\",\n",
    "            target=\"AffiliationId_2\",\n",
    "            edge_attr=edge_attr,\n",
    "            create_using=nx.Graph()\n",
    "        )\n",
    "        if G.number_of_edges() == 0:\n",
    "            # should not hit\n",
    "            tmp = gdf.copy()\n",
    "            tmp[out_col] = 0.0\n",
    "            out_chunks.append(tmp)\n",
    "            continue\n",
    "\n",
    "        ent_map = {}\n",
    "        for u, v in G.edges():\n",
    "            # neighbors around u and v (excluding the counterpart)\n",
    "            wu = [float(G[u][nbr].get(weight_col, 1.0)) for nbr in G.neighbors(u) if nbr != v]\n",
    "            wv = [float(G[v][nbr].get(weight_col, 1.0)) for nbr in G.neighbors(v) if nbr != u]\n",
    "            ent_map[(min(u, v), max(u, v))] = _shannon_entropy_bits(wu + wv)\n",
    "\n",
    "        tmp = gdf.copy()\n",
    "        tmp[out_col] = [\n",
    "            ent_map.get((min(u, v), max(u, v)), 0.0)\n",
    "            for u, v in zip(tmp[\"AffiliationId_1\"], tmp[\"AffiliationId_2\"])\n",
    "        ]\n",
    "        out_chunks.append(tmp)\n",
    "\n",
    "    return pd.concat(out_chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd7f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges_df, nodes_df = build_interlock_graphs(board_df)\n",
    "# edges_df = attach_weakness(edges_df, nodes_df)\n",
    "\n",
    "# # Add simple aliases for convenience\n",
    "# edges_df = edges_df.assign(\n",
    "#     Source=edges_df[\"AffiliationId_1\"],\n",
    "#     Target=edges_df[\"AffiliationId_2\"],\n",
    "#     Weight=edges_df[\"WeightNorm\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d74aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\networkx\\algorithms\\centrality\\eigenvector.py:352: RuntimeWarning: k >= N - 1 for N * N square matrix. Attempting to use scipy.linalg.eig instead.\n",
      "  _, eigenvector = sp.sparse.linalg.eigs(\n"
     ]
    }
   ],
   "source": [
    "# same as before\n",
    "edges_df, nodes_df = build_interlock_graphs(board_df)\n",
    "edges_df = attach_weakness(edges_df, nodes_df)\n",
    "\n",
    "# your aliases\n",
    "edges_df = edges_df.assign(\n",
    "    Source=edges_df[\"AffiliationId_1\"],\n",
    "    Target=edges_df[\"AffiliationId_2\"],\n",
    "    Weight=edges_df[\"WeightNorm\"]\n",
    ")\n",
    "\n",
    "# NEW: put all node metrics directly into nodes_df (now by Year)\n",
    "nodes_df = attach_node_metrics(\n",
    "    edges_df, nodes_df,\n",
    "    weight_col=\"Weight\",            # your alias, or \"WeightNorm\"\n",
    "    use_weighted_betweenness=False  # set True to use distance = 1/weight\n",
    ")\n",
    "\n",
    "# OPTIONAL: add edge entropy to edges_df\n",
    "edges_df = attach_edge_entropy(edges_df, weight_col=\"Weight\", out_col=\"EdgeEntropyBits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b778967",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.to_csv(os.path.join(data_path, 'interlocks', 'interlock_edges.csv'), index=False)\n",
    "nodes_df.to_csv(os.path.join(data_path, 'interlocks', 'interlock_nodes.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba7bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df, nodes_df = build_interlock_graphs(board_df)\n",
    "edges_df = attach_weakness(edges_df, nodes_df)\n",
    "\n",
    "# Add simple aliases for convenience\n",
    "edges_df = edges_df.assign(\n",
    "    Source=edges_df[\"AffiliationId_1\"],\n",
    "    Target=edges_df[\"AffiliationId_2\"],\n",
    "    Weight=edges_df[\"WeightNorm\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
